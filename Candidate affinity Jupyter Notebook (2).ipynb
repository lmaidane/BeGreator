{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "import docx2txt\n",
    "from geopy.geocoders import Nominatim\n",
    "from geotext import GeoText\n",
    "from geopy.distance import great_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skills data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp',\n",
       " 'user interface',\n",
       " 'vmware',\n",
       " 'cx',\n",
       " 'hive',\n",
       " 'scikit learn',\n",
       " 'data warehousing',\n",
       " 'xgboost',\n",
       " 'software development',\n",
       " 'azure',\n",
       " '.net',\n",
       " 'firewalls',\n",
       " 'sharepoint',\n",
       " 'iq',\n",
       " 'apache',\n",
       " 'azure cloud',\n",
       " 'rest/soap api',\n",
       " 'dtp',\n",
       " 'business analysis',\n",
       " 'java',\n",
       " 'artifactory',\n",
       " 'flask',\n",
       " 'numba',\n",
       " 'rdbms',\n",
       " 'kano model',\n",
       " 'koin',\n",
       " 'vuejs',\n",
       " 'blueprism',\n",
       " 'neural network',\n",
       " 'backend development',\n",
       " 'optimisation technique',\n",
       " 'django',\n",
       " 'e-commerce',\n",
       " 'ux',\n",
       " 'data engineering',\n",
       " 'digital finance',\n",
       " 'devops',\n",
       " 'impala',\n",
       " 'pyramid',\n",
       " 'dask',\n",
       " 'powerbi',\n",
       " 'node js',\n",
       " 'citrix',\n",
       " 'scrum',\n",
       " 'hyper-v',\n",
       " 'salesforce',\n",
       " 'information retrieval',\n",
       " 'snowflake',\n",
       " 'telecommunication',\n",
       " 'http',\n",
       " 'ecs',\n",
       " 'restful',\n",
       " 'agile',\n",
       " 'conda',\n",
       " 'stata',\n",
       " 'zeplin',\n",
       " 'aris',\n",
       " 'ux/ui',\n",
       " 'regression',\n",
       " 'gocd',\n",
       " 'numpy',\n",
       " 'api',\n",
       " 'blockchain',\n",
       " 'oracle db',\n",
       " 'visual basic',\n",
       " 'pfmea',\n",
       " 'athena',\n",
       " 'data curation',\n",
       " 'nosql',\n",
       " 'scala',\n",
       " 'cqrs',\n",
       " 'pl/sql',\n",
       " 'ruby',\n",
       " 'rest',\n",
       " 'js',\n",
       " 'xamarin',\n",
       " 'animation design',\n",
       " 'angular',\n",
       " 'functional analysis',\n",
       " 'kafka',\n",
       " 'lambda',\n",
       " 'quantitative analysis',\n",
       " 'c',\n",
       " 'php',\n",
       " 'golang',\n",
       " 'apache poi',\n",
       " 'keyshot',\n",
       " 'crm',\n",
       " 'nginx',\n",
       " 'css3',\n",
       " 'xpath',\n",
       " 'data mining',\n",
       " 'data infrastructure',\n",
       " 'bpm',\n",
       " 'tensorflow',\n",
       " 'interaction design',\n",
       " 'gcp',\n",
       " 'firewall',\n",
       " 'bi',\n",
       " 'time series modelling',\n",
       " 'vue js',\n",
       " 'graph database',\n",
       " 'mvvm',\n",
       " 'graphical editing',\n",
       " 'power bi',\n",
       " 'dbms',\n",
       " 'data manipulation',\n",
       " 'claim management',\n",
       " 'maven',\n",
       " 'tensor flow',\n",
       " 'abode suite',\n",
       " 'oracle',\n",
       " 'tomcat',\n",
       " 'artificial intelligence',\n",
       " 'data analytical',\n",
       " 'ml',\n",
       " 'mining',\n",
       " 'android sdk',\n",
       " 'problem solving',\n",
       " 'nexus',\n",
       " 'image recognition systems',\n",
       " 'git',\n",
       " 'spring',\n",
       " 'html',\n",
       " 'finance',\n",
       " 'cloud computing',\n",
       " 'json',\n",
       " 'rpa',\n",
       " 'ui',\n",
       " 'android studio',\n",
       " 'c-sharp',\n",
       " 'react js',\n",
       " 'prototype design',\n",
       " 'sap',\n",
       " 'digital design',\n",
       " 'no sql',\n",
       " 'mongo db',\n",
       " 'excel',\n",
       " 'wcag aa',\n",
       " 'figma',\n",
       " 'flutter',\n",
       " 'spss',\n",
       " 'process modelling',\n",
       " 'matlab',\n",
       " 'rust',\n",
       " 'itil',\n",
       " 'predictive analytics',\n",
       " 'embedded development',\n",
       " 'invision',\n",
       " 'jax-rs',\n",
       " 'business process',\n",
       " 'redis',\n",
       " 'tornado',\n",
       " 'probability',\n",
       " 'digital transformation',\n",
       " 'feature engineering',\n",
       " 'fat',\n",
       " 'tdd',\n",
       " 'vba',\n",
       " 'talend',\n",
       " 'mobx',\n",
       " 'visualizations',\n",
       " 'confluence',\n",
       " 'adobe illustrator',\n",
       " 'image processing',\n",
       " 'go',\n",
       " 'statistics',\n",
       " 'application development',\n",
       " 'data validation',\n",
       " 'adobe cc',\n",
       " 'autosar',\n",
       " 'cism',\n",
       " 'jenkins',\n",
       " 'application integration',\n",
       " 'spring boot',\n",
       " 'enterprise/solution architecture',\n",
       " 'glue',\n",
       " 'seo',\n",
       " 'hbase',\n",
       " 'transfer learning',\n",
       " 'github',\n",
       " 'bitbucket',\n",
       " 'data visualizations',\n",
       " 'data vault',\n",
       " 'madcap flare',\n",
       " 'selenium',\n",
       " 'powershell',\n",
       " 'kubernetes',\n",
       " 'lightroom',\n",
       " 'microsoft word',\n",
       " 'abstract',\n",
       " 'swift',\n",
       " 'back-end development',\n",
       " 'wireframing',\n",
       " 'computer vision',\n",
       " 'tableau',\n",
       " 'ai',\n",
       " 'redshift',\n",
       " 'wireshark',\n",
       " 'cms',\n",
       " 'ionic',\n",
       " 'predictive modelling',\n",
       " 'node',\n",
       " 'photoshop',\n",
       " 'mvp',\n",
       " 'spark',\n",
       " 'sqs',\n",
       " 'polymer',\n",
       " 'data cleaning',\n",
       " 'microsoft office',\n",
       " 'analytics',\n",
       " 'sat',\n",
       " 'solution design',\n",
       " 'sass',\n",
       " 'docker',\n",
       " 'wcf',\n",
       " 'nodejs',\n",
       " 'ios',\n",
       " 'saml2',\n",
       " 'ansible',\n",
       " 'adfs',\n",
       " 'informatica',\n",
       " 'animation',\n",
       " 'hadoop',\n",
       " 'information architecture',\n",
       " 'data visualization',\n",
       " 'hdfs',\n",
       " 'keras',\n",
       " 'visualisations',\n",
       " 'svn',\n",
       " 'qlik',\n",
       " 'adobe suite',\n",
       " 'android',\n",
       " 'html 5',\n",
       " 'uml',\n",
       " 'typescript',\n",
       " 'pmi',\n",
       " 'supervised and unsupervised',\n",
       " 'program management',\n",
       " 'joomla',\n",
       " 'microsoft excel',\n",
       " 'ms word',\n",
       " 'apache hadoop',\n",
       " 'data visualisation',\n",
       " 'angularjs',\n",
       " 'jira',\n",
       " 'prince 2',\n",
       " 'mapreduce',\n",
       " 'swarn',\n",
       " 'elasticsearch',\n",
       " 'data analysis',\n",
       " 'saas',\n",
       " 'iot',\n",
       " 'mvc',\n",
       " 'mysql',\n",
       " 'big data',\n",
       " 'data interpretation',\n",
       " 'ms project',\n",
       " 'visualisation',\n",
       " 'natural language processing',\n",
       " 'spark/hadoop',\n",
       " 'mesos',\n",
       " 'presto',\n",
       " 'wiki',\n",
       " 'pytorch',\n",
       " 'sketch',\n",
       " 'vsts',\n",
       " 'business intelligence',\n",
       " 'application architecture',\n",
       " 'eclipse',\n",
       " 'xslt',\n",
       " 'scss',\n",
       " 'digital marketing',\n",
       " 'graphic design',\n",
       " 'node.js',\n",
       " 'lua',\n",
       " 'data lakes',\n",
       " 'oee',\n",
       " 'elastic search',\n",
       " 'machine learning',\n",
       " 'modelling',\n",
       " 'version control',\n",
       " 'c#.net',\n",
       " 'javascript',\n",
       " 'product management',\n",
       " 'mip',\n",
       " 'html5',\n",
       " 'mariadb',\n",
       " 'lightgbm',\n",
       " 'redux',\n",
       " 'pandas',\n",
       " 'troubleshooting',\n",
       " 'perl',\n",
       " 'soap',\n",
       " 'nonaws',\n",
       " 'flink',\n",
       " 'ms sql',\n",
       " 'web analytics',\n",
       " 'mongodb',\n",
       " 'neo4j',\n",
       " 'scipy',\n",
       " 'graph theory',\n",
       " 'sabsa',\n",
       " 'drupal',\n",
       " 'cloud architecture',\n",
       " 'project management',\n",
       " 'graphql',\n",
       " 'bigquery',\n",
       " 'big table sql',\n",
       " 'process management',\n",
       " 'rest api',\n",
       " 'adobe',\n",
       " 'jest',\n",
       " 'risk management',\n",
       " 'fortify',\n",
       " 'sql(oracle)',\n",
       " 'python',\n",
       " 'circleci',\n",
       " 'optimization technique',\n",
       " 'vector',\n",
       " 'torch',\n",
       " 'bpmn',\n",
       " 'wire-framing',\n",
       " 'haskell',\n",
       " 'c/c++',\n",
       " 'cloud',\n",
       " 'business process management',\n",
       " 'image recognition system',\n",
       " 'c++',\n",
       " 'deep learning',\n",
       " 'analysis',\n",
       " 'swagger',\n",
       " 'apache beam',\n",
       " 'aws',\n",
       " 'visualization',\n",
       " 'react',\n",
       " 'digital photography',\n",
       " 'bioinformatics',\n",
       " 'angular js',\n",
       " 'ir',\n",
       " 'hpalm',\n",
       " 'power query',\n",
       " 's3',\n",
       " 'deployment',\n",
       " 'esb',\n",
       " 'data vizualisation',\n",
       " 'storm',\n",
       " 'indesign',\n",
       " 'enterprise architecture',\n",
       " 'react.js',\n",
       " 'visual design',\n",
       " 'user experience',\n",
       " 'dynamo db',\n",
       " 'neural networks',\n",
       " 'jupyter',\n",
       " 'computer architecture',\n",
       " 'cgp',\n",
       " 'dax',\n",
       " 'web technology',\n",
       " 'complex modelling',\n",
       " 'optimisation techniques',\n",
       " 'non aws',\n",
       " 'r',\n",
       " 'pip',\n",
       " 'akka',\n",
       " 'cuda',\n",
       " 'framer',\n",
       " 'sparkr',\n",
       " 'data extraction',\n",
       " 'cad',\n",
       " 'web design',\n",
       " 'nlp',\n",
       " 'sea',\n",
       " 'software testing',\n",
       " 'jvm',\n",
       " 'pyspark',\n",
       " 'data regression analysis',\n",
       " 'database administration',\n",
       " 'xml',\n",
       " 'sql',\n",
       " 'hibernate',\n",
       " 'solution architecture',\n",
       " 'process validation',\n",
       " 'word',\n",
       " 'c#',\n",
       " 'sqoop',\n",
       " 'omnichannel',\n",
       " 'adobe xd',\n",
       " 'creo',\n",
       " 'vue.js',\n",
       " 'wordpress',\n",
       " 'data warehouse',\n",
       " 'jquery',\n",
       " 'google cloud platform',\n",
       " 'bigtable sql',\n",
       " 'retrofit',\n",
       " 'ant',\n",
       " 'reactjs',\n",
       " 'elastic',\n",
       " 'c#net',\n",
       " 'solidworks',\n",
       " 'data visualisations',\n",
       " 'statistical',\n",
       " 'illustrator',\n",
       " 'sas',\n",
       " 'graph databases',\n",
       " 'sagemaker',\n",
       " 'openshift',\n",
       " 'data analytics',\n",
       " 'optimization techniques',\n",
       " 'microsoft bot',\n",
       " 'net c#',\n",
       " 'oozie',\n",
       " 'ci/cd',\n",
       " 'power pivot',\n",
       " 'embedded system',\n",
       " 'sonar',\n",
       " 'pmbok',\n",
       " 'unix',\n",
       " 'quality assurance',\n",
       " 'data modelling',\n",
       " 'data preprocessing',\n",
       " 'css',\n",
       " 'arduino',\n",
       " 'cassandra',\n",
       " 'video editing',\n",
       " 'etl',\n",
       " 'net',\n",
       " 'bash',\n",
       " 'sbt',\n",
       " 'prototyping',\n",
       " 'postgres',\n",
       " 'kotlin',\n",
       " 'object oriented design',\n",
       " 'arm',\n",
       " 'powerpoint',\n",
       " 'networks',\n",
       " 'google cloud',\n",
       " 'restapi',\n",
       " 'linux',\n",
       " 'visual composition',\n",
       " 'bamboo',\n",
       " 'product design',\n",
       " 'jhipster',\n",
       " 'analytical',\n",
       " 'app development',\n",
       " 'office 365']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading skills csv file\n",
    "skill_data=pd.read_csv(\"skills_lower1 - Sheet1.csv\")\n",
    "\n",
    "#Collecting skills\n",
    "skill_list=[]\n",
    "for i in range(2262):\n",
    "    if(skill_data['skill_or_not'][i]==\"skill\"):\n",
    "        skill_list.append(skill_data['essential:'][i])\n",
    "                \n",
    "#Converting skills data into unique list of skills\n",
    "unique_skill_list=set(skill_list)\n",
    "unique_skill_list=list(unique_skill_list)\n",
    "unique_skill_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Languages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danish',\n",
       " 'dutch',\n",
       " 'french',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'german',\n",
       " 'haitian creole',\n",
       " 'indonesian',\n",
       " 'malay',\n",
       " 'swahili',\n",
       " 'albanian',\n",
       " 'amharic',\n",
       " 'armenian',\n",
       " 'azerbaijani',\n",
       " 'bengali',\n",
       " 'bulgarian',\n",
       " 'burmese',\n",
       " 'czech',\n",
       " 'dari',\n",
       " 'estonian',\n",
       " 'farsi',\n",
       " 'finnish',\n",
       " 'georgian',\n",
       " 'greek',\n",
       " 'gujarati',\n",
       " 'hausa',\n",
       " 'hebrew',\n",
       " 'hindi',\n",
       " 'hungarian',\n",
       " 'icelandic',\n",
       " 'kazakh',\n",
       " 'khmer',\n",
       " 'kurdish',\n",
       " 'kyrgyz',\n",
       " 'lao',\n",
       " 'latvian',\n",
       " 'lithuanian',\n",
       " 'macedonian',\n",
       " 'mongolian',\n",
       " 'nepali',\n",
       " 'pashto',\n",
       " 'polish',\n",
       " 'russian',\n",
       " 'serbo-croatian',\n",
       " 'sinhala',\n",
       " 'slovak',\n",
       " 'slovenian',\n",
       " 'somali',\n",
       " 'tagalog',\n",
       " 'tajiki',\n",
       " 'tamil',\n",
       " 'telugu',\n",
       " 'thai',\n",
       " 'tibetan',\n",
       " 'turkish',\n",
       " 'turkmen',\n",
       " 'ukranian',\n",
       " 'urdu',\n",
       " 'uzbek',\n",
       " 'vietnamese',\n",
       " 'arabic',\n",
       " 'chinese - cantonese',\n",
       " 'chinese - mandarin',\n",
       " 'japanese',\n",
       " 'korean']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading languages csv file\n",
    "language_data=pd.read_csv(\"languages.csv\")\n",
    "\n",
    "#Collecting languages and converting to list\n",
    "language_list=language_data['Language']\n",
    "language_list=list(language_list)\n",
    "\n",
    "#All the languages in the list are converted to lower case (normalization)\n",
    "for i in range(len(language_list)):\n",
    "    language_list[i]=language_list[i].lower()\n",
    "language_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data of different names of the skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file of different namings for the skills\n",
    "diff_naming=pd.read_csv(\"skill_naming1.csv\")\n",
    "\n",
    "#Converted into a dataframe\n",
    "diff_naming_df=pd.DataFrame(diff_naming)\n",
    "\n",
    "#The incorrect naming is stored in wrong \n",
    "wrong=diff_naming_df['wrong']\n",
    "\n",
    "#The proper naming is stored in correct for the corresponding skills\n",
    "correct=diff_naming_df['correct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Job information Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the job information file scrapped from linkedin\n",
    "job_information_data = pd.read_csv(\"Data jobs.csv\")\n",
    "job_information_dataframe=pd.DataFrame(job_information_data)\n",
    "\n",
    "#Can enter a particular row for the corresponding job informations \n",
    "single_company_data=job_information_dataframe.iloc[1600]\n",
    "\n",
    "#Job Description\n",
    "job_description_data=single_company_data['Description']\n",
    "\n",
    "#Location of the company\n",
    "company_location=single_company_data['Location']\n",
    "\n",
    "#Industry type\n",
    "industry=single_company_data['Industry']\n",
    "\n",
    "#Seniority level\n",
    "level=single_company_data['Level']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(data):\n",
    "    \n",
    "    #Removing punctuations from the text\n",
    "    cleaned_data = re.sub(r'[,.;'':@#?!&$()/]', ' ', data)\n",
    "    \n",
    "    #tokenization of the text\n",
    "    tokenized_data = nltk.word_tokenize(cleaned_data)\n",
    "    \n",
    "    #Removing Stop words\n",
    "    filtered_words = [word for word in tokenized_data if word not in stopwords.words('english')]\n",
    "    \n",
    "    #All the unigram words are converted to lower case\n",
    "    for i in range(len(filtered_words)):\n",
    "        filtered_words[i] = filtered_words[i]. lower()\n",
    "        \n",
    "    #Bigram of words\n",
    "    bigram_data=list(nltk.bigrams(tokenized_data))\n",
    "    final_words=list()\n",
    "    for i in bigram_data:\n",
    "        test_string=''\n",
    "        test_string=' '.join(i)\n",
    "        final_words.append(test_string)\n",
    "\n",
    "    #Trigram of words\n",
    "    trigram_data=list(nltk.trigrams(tokenized_data))\n",
    "    for i in trigram_data:\n",
    "        test_string=''\n",
    "        test_string=' '.join(i)\n",
    "        final_words.append(test_string)\n",
    "    \n",
    "    #Collection of unigrams, bigrams, trigrams\n",
    "    for i in filtered_words:    \n",
    "        final_words.append(i)\n",
    "    \n",
    "    #All words are converted to lower case\n",
    "    for i in range(len(final_words)):\n",
    "        final_words[i] = final_words[i]. lower()\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding different types of analytics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_types_analytics(final_words):\n",
    "    \n",
    "    #Collects the previous word of \"analytics\" in the list of words\n",
    "    string=''\n",
    "    for i in range(len(final_words)):\n",
    "        if(final_words[i]==\"analytics\"):\n",
    "            string=string+' '+(str(final_words[i-1]))\n",
    "    \n",
    "    #tokenization\n",
    "    text = nltk.word_tokenize(string)\n",
    "                              \n",
    "    #POS_tagging\n",
    "    #Tells for all the words, whether it is verb, noun, adjective, etc\n",
    "    pos_tagged_text=nltk.pos_tag(text)\n",
    "    tagged_words = nltk.ConditionalFreqDist((tag, word) for (word, tag) in pos_tagged_text)\n",
    "    \n",
    "    #Mentioned the types of noun taggings\n",
    "    noun=['NN','NN$','NN$-HL','NN$-TL','NN-HL','NN-NC','NN-TL','NN-TL-HL','NNS','NNS$','NNS$-HL','NNS$-TL','NNS-HL','NNS-TL','NNS-TL-HL']\n",
    "    \n",
    "    #Collects the previous words of 'analytics' which are noun\n",
    "    final_analytics=[]\n",
    "    for i in range(len(noun)):\n",
    "        for key in tagged_words[noun[i]].keys():\n",
    "            final_analytics.append(key)\n",
    "    \n",
    "    #Appending those noun words like 'data', 'predictive' with 'analytics' ==> 'data analytics', 'predictive analytics'\n",
    "    for i in range(len(final_analytics)):\n",
    "        final_analytics[i]=str(final_analytics[i])+' '+\"analytics\"\n",
    "    return final_analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting languages from a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def languages(final_words):\n",
    "    #from the list of words, collects the languages\n",
    "    language=[]\n",
    "    for i in range(len(final_words)):\n",
    "        for j in range(len(language_list)):\n",
    "            if(final_words[i]==language_list[j]):\n",
    "                language.append(language_list[j])\n",
    "                \n",
    "    #converting it into unique list\n",
    "    language=set(language)\n",
    "    language=list(language)\n",
    "    #if(len(language)==0):\n",
    "    #    return \"No languages mentioned\"\n",
    "    #else:\n",
    "    return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding difference between R language and R&D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffbet_Rlang_RndD(final_words):\n",
    "    r_count=0\n",
    "    rd_count=0\n",
    "    for i in range(len(final_words)):\n",
    "        if(final_words[i]==\"r\"):\n",
    "            r_count=r_count+1\n",
    "            if(i==len(final_words)-1):\n",
    "                break;\n",
    "            elif(final_words[i+1]=='d'):\n",
    "                rd_count=rd_count+1\n",
    "    if(rd_count>0):\n",
    "        if(r_count==rd_count):\n",
    "            final_words.remove(\"r\")\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting list of skills from the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_skill_list(concluded_words,unique_skill_list):\n",
    "    matching_skills=[]\n",
    "    for i in range(len(concluded_words)):\n",
    "        for j in range(len(unique_skill_list)):\n",
    "            if(concluded_words[i]==unique_skill_list[j]):\n",
    "                matching_skills.append(concluded_words[i])\n",
    "    \n",
    "    matching_skills=set(matching_skills)\n",
    "    matching_skills=list(matching_skills)\n",
    "    return matching_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the incorrectly mentioned skill and appending the skill list with correct name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_wrong_namings(matching_skills,wrong,correct):\n",
    "    for i in range(len(matching_skills)):\n",
    "        for j in range(len(wrong)):\n",
    "            if(matching_skills[i]==wrong[j]):\n",
    "                matching_skills.remove(wrong[j])\n",
    "                matching_skills.append(correct[j])\n",
    "    \n",
    "    matching_skills=set(matching_skills)\n",
    "    matching_skills=list(matching_skills)\n",
    "    return matching_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding difference between 'analytics' and different types of analytics like 'data analytics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_skills(matching_skills,corrected_skills,final_analytics):\n",
    "    diff_analytics_count=0\n",
    "    analytics_count=0\n",
    "    for i in range(len(matching_skills)):\n",
    "        if(matching_skills[i]==\"analytics\"):\n",
    "            analytics_count=analytics_count+1\n",
    "        for j in range(len(final_analytics)):\n",
    "            if(matching_skills[i]==final_analytics[j]):\n",
    "                diff_analytics_count=diff_analytics_count+1\n",
    "    if(diff_analytics_count>0):\n",
    "        if(analytics_count==diff_analytics_count):\n",
    "            corrected_skills.remove(\"analytics\")\n",
    "    return corrected_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concluding informations from job data by calling the above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the above functions\n",
    "words_job_description=processing(job_description_data)\n",
    "\n",
    "final_analytics_jd=diff_types_analytics(words_job_description)    \n",
    "final_languages_jd=languages(words_job_description)\n",
    "concluded_words_jd=diffbet_Rlang_RndD(words_job_description)\n",
    "\n",
    "matching_skills_jd=matching_skill_list(concluded_words_jd,unique_skill_list)\n",
    "corrected_skills_jd=remove_wrong_namings(matching_skills_jd,wrong,correct)\n",
    "processed_skills_jd=processing_skills(matching_skills_jd,corrected_skills_jd,final_analytics_jd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industry Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading industries names file\n",
    "names_data=pd.read_csv(\"industries1.csv\")\n",
    "names_dataframe=pd.DataFrame(names_data)\n",
    "one=names_dataframe['one']\n",
    "two=names_dataframe['two']\n",
    "three=names_dataframe['three']\n",
    "four=names_dataframe['four']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['   ']\n"
     ]
    }
   ],
   "source": [
    "#making changes in the words which are wrongly displayed\n",
    "#eg.: SoftwareStaffingRecruiting => Software Staffing Recruting\n",
    "ind=[]\n",
    "\n",
    "string=''\n",
    "for j in industry:\n",
    "    if(j==j.lower()):\n",
    "        string=string+j\n",
    "    if(j==j.upper()):\n",
    "        string = string+\" \"+j\n",
    "ind.append(string)\n",
    "\n",
    "for i in range(len(ind)):\n",
    "    if(\"& &\" in ind[i]):\n",
    "        ind[i]=ind[i].replace(\" & \",' ')\n",
    "    if(\", ,\" in ind[i]):\n",
    "        ind[i]=ind[i].replace(\", ,\",' ')\n",
    "    if(\"/\" in ind[i]):\n",
    "        ind[i]=ind[i].replace(\"/\",' ')\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization        \n",
    "for i in range(len(ind)):\n",
    "    test=nltk.word_tokenize(ind[i])\n",
    "\n",
    "    #finds the industry type of the company    \n",
    "    if(len(test)==1):\n",
    "        for l in range(len(one)):\n",
    "            if(test[0]==one[l]):\n",
    "                print(test[0])\n",
    "\n",
    "    elif(len(test)==2):\n",
    "        for l in range(len(one)):\n",
    "            if(test[0]==one[l] and test[1]==two[l]):\n",
    "                print(test[0],test[1])\n",
    "            elif(test[1]==one[l] and two[l]==' '):\n",
    "                print(test[1])\n",
    "\n",
    "    elif(len(test)==3):\n",
    "        for l in range(len(one)):\n",
    "            if(test[0]==one[l] and test[1]==two[l] and test[2]==three[l]):\n",
    "                print(test[0],test[1],test[2]) \n",
    "            elif(test[1]==one[l] and test[2]==two[l]):\n",
    "                print(test[1],test[2]) \n",
    "            elif(test[2]==one[l]):\n",
    "                print(test[2]) \n",
    "\n",
    "    elif(len(test)==0):\n",
    "        continue;\n",
    "\n",
    "    else:\n",
    "        ind1=[]\n",
    "        for j in range(len(test),len(test)-4,-1):\n",
    "            ind1.append(test[j-1])\n",
    "        ind1=ind1[::-1]\n",
    "        for l in range(len(one)):\n",
    "            if(ind1[0]==one[l] and ind1[1]==two[l] and ind1[2]==three[l] and ind1[3]==four[l]):\n",
    "                print(ind1[0],ind1[1],ind1[2],ind1[3]) \n",
    "            elif(ind1[1]==one[l] and ind1[2]==two[l] and ind1[3]==three[l]):\n",
    "                print(ind1[1],ind1[2],ind1[3]) \n",
    "            elif(ind1[2]==one[l] and ind1[3]==two[l]):\n",
    "                print(ind1[2],ind1[3]) \n",
    "            elif(ind1[3]==one[l]):\n",
    "                print(ind1[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputting informations from the resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID                                                            NaN\n",
      "Date                                                     30+ days ago\n",
      "Company Name                                                    Smals\n",
      "Post                                                   Data Scientist\n",
      "Location                                                     Brussels\n",
      "No.of Applicants                                                     \n",
      "Description         Chez Smals, plus de 1900 professionnels façonn...\n",
      "Level                                                                \n",
      "Type                                                                 \n",
      "Function                                                             \n",
      "Industry                                                             \n",
      "Link                https://be.indeed.com/pagead/clk?mo=r&ad=-6NYl...\n",
      "Review                                                            3.3\n",
      "Name: 1500, dtype: object\n",
      "\n",
      "Required languages:\n",
      "[]\n",
      "\n",
      "Required skills:\n",
      "['r', 'big data', 'python', 'sas']\n",
      "\n",
      "\n",
      "Distance in km:\n",
      "Geoffrey Brown.docx : 48.75007126589204\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Geoffrey Brown.docx : []\n",
      "Experience level : 4\n",
      "level matches\n",
      "\n",
      "\n",
      "Distance in km:\n",
      "LyanneGibson.docx : 48.75007126589204\n",
      "Languages known : ['french']\n",
      "Matching languages : \n",
      "LyanneGibson.docx : []\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#skills and languages are collected from the job description\n",
    "print(single_company_data)\n",
    "print(\"\\nRequired languages:\")\n",
    "print(final_languages_jd)\n",
    "print(\"\\nRequired skills:\")\n",
    "print(processed_skills_jd)\n",
    "print(\"\\n\")\n",
    "\n",
    "candidate_skills=[]\n",
    "candidate={}\n",
    "a,b,c=0,0,0\n",
    "name=[]\n",
    "\n",
    "#Reading all the resumes from the 'resumes' folder\n",
    "entries = os.listdir('resumes')\n",
    "\n",
    "#for loop for each resume\n",
    "for entry in entries:\n",
    "    \n",
    "    #converts docs file to text\n",
    "    resume_data = docx2txt.process(entry)\n",
    "    \n",
    "    #name of the candidate is appended in the list 'name'\n",
    "    name.append(entry)\n",
    "    \n",
    "    #calling the above functions\n",
    "    words_resume=processing(resume_data)\n",
    "    \n",
    "    final_analytics_resume=diff_types_analytics(words_resume)\n",
    "    final_languages_resume=languages(words_resume)\n",
    "    concluded_words_resume=diffbet_Rlang_RndD(words_resume)        \n",
    "    \n",
    "    matching_skills_resume=matching_skill_list(concluded_words_resume,unique_skill_list)\n",
    "    corrected_skills_resume=remove_wrong_namings(matching_skills_resume,wrong,correct)\n",
    "    processed_skills_resume=processing_skills(matching_skills_resume,corrected_skills_resume,final_analytics_resume)\n",
    "    \n",
    "    #skills are collected from the resume\n",
    "    candidate_skills.append(processed_skills_resume)\n",
    "    candidate[name[a]]=processed_skills_resume\n",
    "    a=a+1\n",
    "    \n",
    "    #places = GeoText(resume_data)\n",
    "    #print(places.cities)\n",
    "    #places1=places.cities\n",
    "    #places1=str(places1)\n",
    "\n",
    "    #location of the candidate\n",
    "    candidate_location=\"Charleroi\"\n",
    "    places=[]\n",
    "    places.append(company_location)\n",
    "    places.append(candidate_location)\n",
    "    latitude=[]\n",
    "    longitude=[]\n",
    "      \n",
    "    #finding the distance between the candidate location and the company's location\n",
    "    geolocator = Nominatim(user_agent=\"http\")\n",
    "    for i in range(len(places)):\n",
    "        locate = geolocator.geocode(places[i])\n",
    "        latitude.append(locate.latitude)\n",
    "        longitude.append(locate.longitude)\n",
    "           \n",
    "    first = (latitude[0], longitude[0])\n",
    "    second = (latitude[1], longitude[1])\n",
    "    print(\"Distance in km:\")\n",
    "    print(name[b],\":\",great_circle(first, second).km)\n",
    "    b=b+1\n",
    "    \n",
    "    #collects the languages known by the candidate\n",
    "    #if(len(final_languages_resume)==0):\n",
    "    #    print(\"No languages mentioned\")\n",
    "    #else:\n",
    "    print(\"Languages known :\",final_languages_resume)    \n",
    "    \n",
    "    #finds the matching languages with the job description and the candidate's resume\n",
    "    matching_languages=[]\n",
    "    for i in range(len(final_languages_jd)):\n",
    "        for j in range(len(final_languages_resume)):\n",
    "            if(final_languages_jd[i]==final_languages_resume[j]):\n",
    "                matching_languages.append(final_languages_jd[i])\n",
    "    print(\"Matching languages : \")\n",
    "    print(name[c],\":\",matching_languages)\n",
    "    c=c+1\n",
    "    \n",
    "    #level of experience matching\n",
    "    years=[]\n",
    "    for j in range(len(words_resume)):\n",
    "        if(words_resume[j]=='years' and words_resume[j-2]!='over'):\n",
    "            years.append(words_resume[j-1])\n",
    "    if(len(years)>0):\n",
    "        year=str(years[0])\n",
    "        year=int(''.join(list(filter(lambda c: c.isdigit(), year))))\n",
    "        print(\"Experience level :\",year)\n",
    "    else:\n",
    "        year=0\n",
    "        print(\"Experience level : Not mentioned\")\n",
    "        \n",
    "    \n",
    "    # Job position's needed level        candidate's years of experience  \n",
    "    #-------------------------------------------------------------------\n",
    "    # Not Applicable                =>   greater than or equal to 0\n",
    "    # Entry level or Associate      =>   greater than or equal to 0\n",
    "    # Mid-Senior level              =>   greater than 3\n",
    "    # Executive                     =>   greater than 5\n",
    "    # Director                      =>   greater than 7\n",
    "    \n",
    "    \n",
    "    if(level==\"Not Applicable\" or level==' '):\n",
    "        if(year>=0):\n",
    "            print(\"level matches\")\n",
    "            \n",
    "    elif(level==\"Entry level\" or level==\"Associate\" or 3>int(level)>=0):\n",
    "        if(3>year>=0):\n",
    "            print(\"level matches\")\n",
    "        else:\n",
    "            print(\"level does not match\")\n",
    "            \n",
    "    elif(level==\"Mid-Senior level\" or 5>=int(level)>=3):\n",
    "        if(5>=year>=3):\n",
    "            print(\"level matches\")\n",
    "        else:\n",
    "            print(\"level does not match\")\n",
    "            \n",
    "    elif(level==\"Executive\" or 7>=int(level)>=5):\n",
    "        if(7>=year>=5):\n",
    "            print(\"level matches\")\n",
    "        else:\n",
    "            print(\"level does not match\")  \n",
    "            \n",
    "    elif(level==\"Director\" or int(level)>=7):\n",
    "        if(year>=7):\n",
    "            print(\"level matches\")\n",
    "        else:\n",
    "            print(\"level does not match\")   \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching skills with the job description and the candidate's resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final ranking :  {'Geoffrey Brown.docx': 1, 'LyanneGibson.docx': 2}\n",
      "Matching skills :  {'Geoffrey Brown.docx': {'python'}, 'LyanneGibson.docx': {'r', 'python'}}\n"
     ]
    }
   ],
   "source": [
    "final_rank={}\n",
    "matching={}\n",
    "def fun(candidate,comp,st_n):\n",
    "\n",
    "    set1=set(candidate)\n",
    "    set2=set(comp)\n",
    "    \n",
    "    set3=set1.intersection(set2)\n",
    "    \n",
    "    final_rank.update({st_n:len(set3)})\n",
    "\n",
    "    matching.update({st_n:set3})\n",
    "    \n",
    "for i,j in candidate.items():\n",
    "    fun(processed_skills_jd,j,i)\n",
    "     \n",
    "#Final ranking tells the count of the matching skills \n",
    "print(\"\\nFinal ranking : \" ,final_rank)\n",
    "\n",
    "#Matching skills\n",
    "print('Matching skills : ' , matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
