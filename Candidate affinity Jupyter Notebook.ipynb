{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "import os\n",
    "import docx2txt\n",
    "from geopy.geocoders import Nominatim\n",
    "from geotext import GeoText\n",
    "from geopy.distance import great_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skills data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nginx',\n",
       " 'application architecture',\n",
       " 'creo',\n",
       " 'omnichannel',\n",
       " 'cqrs',\n",
       " 'ai',\n",
       " 'flask',\n",
       " 'apache',\n",
       " 'natural language processing',\n",
       " 'web analytics',\n",
       " 'web design',\n",
       " 'angular js',\n",
       " 'spark/hadoop',\n",
       " 'mvvm',\n",
       " 'jhipster',\n",
       " 'apache beam',\n",
       " 'framer',\n",
       " 'citrix',\n",
       " 'digital transformation',\n",
       " 'tdd',\n",
       " 'php',\n",
       " 'adobe',\n",
       " 'problem solving',\n",
       " 'keras',\n",
       " 'vue js',\n",
       " 'vsts',\n",
       " 'optimisation technique',\n",
       " 'wiki',\n",
       " 'swift',\n",
       " 'regression',\n",
       " 'zeplin',\n",
       " 'postgres',\n",
       " 'python',\n",
       " 'scipy',\n",
       " 'microsoft word',\n",
       " 'data warehouse',\n",
       " 'excel',\n",
       " 'perl',\n",
       " 'sql(oracle)',\n",
       " 'bpm',\n",
       " 'tensorflow',\n",
       " 'artifactory',\n",
       " 'data warehousing',\n",
       " 'complex modelling',\n",
       " 'angular',\n",
       " 'data analysis',\n",
       " 'vuejs',\n",
       " 'microsoft excel',\n",
       " 'rdbms',\n",
       " 'cuda',\n",
       " 'bigquery',\n",
       " 'data extraction',\n",
       " 'product management',\n",
       " 'supervised and unsupervised',\n",
       " 'c++',\n",
       " 'blockchain',\n",
       " 'data lakes',\n",
       " 'invision',\n",
       " 'ui',\n",
       " 'ux',\n",
       " 'android',\n",
       " 'claim management',\n",
       " 'bigtable sql',\n",
       " 'data visualisation',\n",
       " 'oozie',\n",
       " 'c/c++',\n",
       " 'aws',\n",
       " 'machine learning',\n",
       " 'data vizualisation',\n",
       " 'prince 2',\n",
       " 'matlab',\n",
       " 'agile',\n",
       " 'docker',\n",
       " 'lightgbm',\n",
       " 'application development',\n",
       " 'react',\n",
       " 'lambda',\n",
       " 'adobe illustrator',\n",
       " 'visual basic',\n",
       " 'bash',\n",
       " 'node js',\n",
       " 'mongodb',\n",
       " 'uml',\n",
       " 'typescript',\n",
       " 'git',\n",
       " 'cism',\n",
       " 'dtp',\n",
       " 'kotlin',\n",
       " 'ruby',\n",
       " 'nonaws',\n",
       " 'enterprise architecture',\n",
       " 'wordpress',\n",
       " 'blueprism',\n",
       " 'autosar',\n",
       " 'prototyping',\n",
       " 'dbms',\n",
       " 'tornado',\n",
       " 'visualization',\n",
       " 'fortify',\n",
       " 'cgp',\n",
       " 'seo',\n",
       " 'time series modelling',\n",
       " 'retrofit',\n",
       " 'computer architecture',\n",
       " 'sap',\n",
       " 'sbt',\n",
       " 'cloud',\n",
       " 'adobe cc',\n",
       " 'spark',\n",
       " 'data interpretation',\n",
       " 'e-commerce',\n",
       " 'adfs',\n",
       " 'data visualizations',\n",
       " 'mip',\n",
       " 'solution architecture',\n",
       " 'mvp',\n",
       " 'pytorch',\n",
       " 'sqoop',\n",
       " 'saas',\n",
       " 'quality assurance',\n",
       " 'mvc',\n",
       " 'feature engineering',\n",
       " 'office 365',\n",
       " 'jira',\n",
       " 'mining',\n",
       " 'sat',\n",
       " 'telecommunication',\n",
       " 'talend',\n",
       " 'wcf',\n",
       " 'dask',\n",
       " 'js',\n",
       " 'embedded system',\n",
       " 'sea',\n",
       " 'predictive analytics',\n",
       " 'polymer',\n",
       " 'indesign',\n",
       " 'swagger',\n",
       " 'scikit learn',\n",
       " 'powershell',\n",
       " 'enterprise/solution architecture',\n",
       " 'scrum',\n",
       " 'data analytics',\n",
       " 'snowflake',\n",
       " 'rust',\n",
       " 'hbase',\n",
       " 'cx',\n",
       " 'bitbucket',\n",
       " 'oee',\n",
       " 'java',\n",
       " 'informatica',\n",
       " 'abstract',\n",
       " 'visualisations',\n",
       " 'digital marketing',\n",
       " 'cassandra',\n",
       " 'node.js',\n",
       " 'madcap flare',\n",
       " 'rest api',\n",
       " 'rest/soap api',\n",
       " 'pip',\n",
       " 'lightroom',\n",
       " 'salesforce',\n",
       " 'ios',\n",
       " 'confluence',\n",
       " 'nodejs',\n",
       " 'image recognition systems',\n",
       " 'jax-rs',\n",
       " 'web technology',\n",
       " 'embedded development',\n",
       " 'artificial intelligence',\n",
       " 'svn',\n",
       " 'wireshark',\n",
       " 'analytics',\n",
       " 'apache poi',\n",
       " 'mapreduce',\n",
       " 'interaction design',\n",
       " 'microsoft bot',\n",
       " 'illustrator',\n",
       " 'data mining',\n",
       " 'openshift',\n",
       " 'itil',\n",
       " 'elastic',\n",
       " 'api',\n",
       " 'modelling',\n",
       " 'qlik',\n",
       " 'saml2',\n",
       " 'elastic search',\n",
       " 'arm',\n",
       " 'impala',\n",
       " 'big table sql',\n",
       " 'risk management',\n",
       " 'tensor flow',\n",
       " 'xpath',\n",
       " 'http',\n",
       " 'wireframing',\n",
       " 'mongo db',\n",
       " 'graph databases',\n",
       " 'graphic design',\n",
       " 'pl/sql',\n",
       " 'prototype design',\n",
       " 'nosql',\n",
       " 'sparkr',\n",
       " 'visualizations',\n",
       " 'c#',\n",
       " 'unix',\n",
       " 'koin',\n",
       " 'iq',\n",
       " 'drupal',\n",
       " 'analytical',\n",
       " 'sagemaker',\n",
       " 'ms sql',\n",
       " 'spring boot',\n",
       " 'cloud architecture',\n",
       " '.net',\n",
       " 'adobe xd',\n",
       " 'oracle db',\n",
       " 'flink',\n",
       " 'wire-framing',\n",
       " 'react js',\n",
       " 'crm',\n",
       " 'power pivot',\n",
       " 'hive',\n",
       " 'c-sharp',\n",
       " 'aris',\n",
       " 'data preprocessing',\n",
       " 'rpa',\n",
       " 'ir',\n",
       " 'restful',\n",
       " 'business intelligence',\n",
       " 'vector',\n",
       " 'xgboost',\n",
       " 'photoshop',\n",
       " 'database administration',\n",
       " 'pmi',\n",
       " 'functional analysis',\n",
       " 'networks',\n",
       " 's3',\n",
       " 'visualisation',\n",
       " 'bamboo',\n",
       " 'ansible',\n",
       " 'ux/ui',\n",
       " 'nexus',\n",
       " 'software development',\n",
       " 'react.js',\n",
       " 'non aws',\n",
       " 'firewall',\n",
       " 'hadoop',\n",
       " 'application integration',\n",
       " 'storm',\n",
       " 'sabsa',\n",
       " 'ionic',\n",
       " 'abode suite',\n",
       " 'joomla',\n",
       " 'android sdk',\n",
       " 'neural networks',\n",
       " 'cloud computing',\n",
       " 'node',\n",
       " 'sqs',\n",
       " 'swarn',\n",
       " 'xml',\n",
       " 'pmbok',\n",
       " 'hibernate',\n",
       " 'numba',\n",
       " 'visual composition',\n",
       " 'data analytical',\n",
       " 'deep learning',\n",
       " 'predictive modelling',\n",
       " 'no sql',\n",
       " 'user interface',\n",
       " 'google cloud platform',\n",
       " 'neo4j',\n",
       " 'graph theory',\n",
       " 'ecs',\n",
       " 'powerbi',\n",
       " 'kubernetes',\n",
       " 'torch',\n",
       " 'cad',\n",
       " 'sharepoint',\n",
       " 'data vault',\n",
       " 'pfmea',\n",
       " 'deployment',\n",
       " 'version control',\n",
       " 'net',\n",
       " 'css3',\n",
       " 'solidworks',\n",
       " 'kafka',\n",
       " 'gcp',\n",
       " 'jquery',\n",
       " 'conda',\n",
       " 'software testing',\n",
       " 'akka',\n",
       " 'ci/cd',\n",
       " 'sql',\n",
       " 'vue.js',\n",
       " 'html5',\n",
       " 'process modelling',\n",
       " 'statistical',\n",
       " 'figma',\n",
       " 'animation design',\n",
       " 'vmware',\n",
       " 'business process',\n",
       " 'cms',\n",
       " 'nlp',\n",
       " 'jupyter',\n",
       " 'circleci',\n",
       " 'graph database',\n",
       " 'object oriented design',\n",
       " 'golang',\n",
       " 'information architecture',\n",
       " 'statistics',\n",
       " 'flutter',\n",
       " 'spring',\n",
       " 'data curation',\n",
       " 'elasticsearch',\n",
       " 'esb',\n",
       " 'iot',\n",
       " 'reactjs',\n",
       " 'hpalm',\n",
       " 'graphql',\n",
       " 'backend development',\n",
       " 'data cleaning',\n",
       " 'powerpoint',\n",
       " 'athena',\n",
       " 'xslt',\n",
       " 'haskell',\n",
       " 'c',\n",
       " 'tableau',\n",
       " 'data engineering',\n",
       " 'optimisation techniques',\n",
       " 'jvm',\n",
       " 'image recognition system',\n",
       " 'redshift',\n",
       " 'graphical editing',\n",
       " 'pyspark',\n",
       " 'animation',\n",
       " 'optimization techniques',\n",
       " 'data visualisations',\n",
       " 'gocd',\n",
       " 'app development',\n",
       " 'lua',\n",
       " 'hyper-v',\n",
       " 'fat',\n",
       " 'tomcat',\n",
       " 'mesos',\n",
       " 'power query',\n",
       " 'data infrastructure',\n",
       " 'soap',\n",
       " 'analysis',\n",
       " 'keyshot',\n",
       " 'troubleshooting',\n",
       " 'scss',\n",
       " 'mobx',\n",
       " 'cp',\n",
       " 'android studio',\n",
       " 'glue',\n",
       " 'solution design',\n",
       " 'data modelling',\n",
       " 'redux',\n",
       " 'ms project',\n",
       " 'arduino',\n",
       " 'power bi',\n",
       " 'google cloud',\n",
       " 'c#.net',\n",
       " 'business process management',\n",
       " 'word',\n",
       " 'oracle',\n",
       " 'jenkins',\n",
       " 'apache hadoop',\n",
       " 'data validation',\n",
       " 'c#net',\n",
       " 'mysql',\n",
       " 'business analysis',\n",
       " 'back-end development',\n",
       " 'presto',\n",
       " 'sketch',\n",
       " 'sonar',\n",
       " 'vba',\n",
       " 'optimization technique',\n",
       " 'bioinformatics',\n",
       " 'django',\n",
       " 'azure cloud',\n",
       " 'probability',\n",
       " 'mariadb',\n",
       " 'bi',\n",
       " 'dax',\n",
       " 'bpmn',\n",
       " 'firewalls',\n",
       " 'dynamo db',\n",
       " 'xamarin',\n",
       " 'information retrieval',\n",
       " 'sass',\n",
       " 'azure',\n",
       " 'adobe suite',\n",
       " 'spss',\n",
       " 'net c#',\n",
       " 'ms word',\n",
       " 'visual design',\n",
       " 'process management',\n",
       " 'neural network',\n",
       " 'selenium',\n",
       " 'jest',\n",
       " 'sas',\n",
       " 'kano model',\n",
       " 'pyramid',\n",
       " 'project management',\n",
       " 'wcag aa',\n",
       " 'stata',\n",
       " 'user experience',\n",
       " 'quantitative analysis',\n",
       " 'digital design',\n",
       " 'devops',\n",
       " 'product design',\n",
       " 'data regression analysis',\n",
       " 'microsoft office',\n",
       " 'rest',\n",
       " 'javascript',\n",
       " 'data visualization',\n",
       " 'big data',\n",
       " 'hdfs',\n",
       " 'html',\n",
       " 'image processing',\n",
       " 'data manipulation',\n",
       " 'ml',\n",
       " 'restapi',\n",
       " 'linux',\n",
       " 'redis',\n",
       " 'maven',\n",
       " 'r',\n",
       " 'angularjs',\n",
       " 'json',\n",
       " 'numpy',\n",
       " 'pandas',\n",
       " 'etl',\n",
       " 'digital finance',\n",
       " 'css',\n",
       " 'eclipse',\n",
       " 'ant',\n",
       " 'finance',\n",
       " 'go',\n",
       " 'program management',\n",
       " 'video editing',\n",
       " 'html 5',\n",
       " 'computer vision',\n",
       " 'digital photography',\n",
       " 'process validation',\n",
       " 'transfer learning',\n",
       " 'github',\n",
       " 'scala']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading skills csv file\n",
    "skill_data=pd.read_csv(\"skills_lower1 - Sheet1.csv\")\n",
    "\n",
    "#Collecting skills\n",
    "skill_list=[]\n",
    "for i in range(2262):\n",
    "    if(skill_data['skill_or_not'][i]==\"skill\"):\n",
    "        skill_list.append(skill_data['essential:'][i])\n",
    "                \n",
    "#Converting skills data into unique list of skills\n",
    "unique_skill_list=set(skill_list)\n",
    "unique_skill_list=list(unique_skill_list)\n",
    "unique_skill_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Languages data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danish',\n",
       " 'dutch',\n",
       " 'french',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'german',\n",
       " 'haitian creole',\n",
       " 'indonesian',\n",
       " 'malay',\n",
       " 'swahili',\n",
       " 'albanian',\n",
       " 'amharic',\n",
       " 'armenian',\n",
       " 'azerbaijani',\n",
       " 'bengali',\n",
       " 'bulgarian',\n",
       " 'burmese',\n",
       " 'czech',\n",
       " 'dari',\n",
       " 'estonian',\n",
       " 'farsi',\n",
       " 'finnish',\n",
       " 'georgian',\n",
       " 'greek',\n",
       " 'gujarati',\n",
       " 'hausa',\n",
       " 'hebrew',\n",
       " 'hindi',\n",
       " 'hungarian',\n",
       " 'icelandic',\n",
       " 'kazakh',\n",
       " 'khmer',\n",
       " 'kurdish',\n",
       " 'kyrgyz',\n",
       " 'lao',\n",
       " 'latvian',\n",
       " 'lithuanian',\n",
       " 'macedonian',\n",
       " 'mongolian',\n",
       " 'nepali',\n",
       " 'pashto',\n",
       " 'polish',\n",
       " 'russian',\n",
       " 'serbo-croatian',\n",
       " 'sinhala',\n",
       " 'slovak',\n",
       " 'slovenian',\n",
       " 'somali',\n",
       " 'tagalog',\n",
       " 'tajiki',\n",
       " 'tamil',\n",
       " 'telugu',\n",
       " 'thai',\n",
       " 'tibetan',\n",
       " 'turkish',\n",
       " 'turkmen',\n",
       " 'ukranian',\n",
       " 'urdu',\n",
       " 'uzbek',\n",
       " 'vietnamese',\n",
       " 'arabic',\n",
       " 'chinese - cantonese',\n",
       " 'chinese - mandarin',\n",
       " 'japanese',\n",
       " 'korean']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading languages csv file\n",
    "language_data=pd.read_csv(\"languages.csv\")\n",
    "\n",
    "#Collecting languages and converting to list\n",
    "language_list=language_data['Language']\n",
    "language_list=list(language_list)\n",
    "\n",
    "#All the languages in the list are converted to lower case (normalization)\n",
    "for i in range(len(language_list)):\n",
    "    language_list[i]=language_list[i].lower()\n",
    "language_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the data of different names of the skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file of different namings for the skills\n",
    "diff_naming=pd.read_csv(\"skill_naming1.csv\")\n",
    "\n",
    "#Converted into a dataframe\n",
    "diff_naming_df=pd.DataFrame(diff_naming)\n",
    "\n",
    "#The incorrect naming is stored in wrong \n",
    "wrong=diff_naming_df['wrong']\n",
    "\n",
    "#The proper naming is stored in correct for the corresponding skills\n",
    "correct=diff_naming_df['correct']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Job information Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scientist\n"
     ]
    }
   ],
   "source": [
    "#Reading the job information file scrapped from linkedin\n",
    "job_information_data = pd.read_csv(\"Data jobs.csv\")\n",
    "job_information_dataframe=pd.DataFrame(job_information_data)\n",
    "\n",
    "#Can enter a particular row for the corresponding job informations \n",
    "single_company_data=job_information_dataframe.iloc[1990]\n",
    "\n",
    "#Job Description\n",
    "job_description_data=single_company_data['Description']\n",
    "\n",
    "#Location of the company\n",
    "company_location=single_company_data['Location']\n",
    "\n",
    "#Industry type\n",
    "industry=single_company_data['Industry']\n",
    "\n",
    "#Seniority level\n",
    "level=single_company_data['Level']\n",
    "\n",
    "#job_title\n",
    "title=single_company_data['Post']\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(data):\n",
    "    \n",
    "    #Removing punctuations from the text\n",
    "    cleaned_data = re.sub(r'[,.;'':@#?!&$()/|]', ' ', data)\n",
    "    \n",
    "    #tokenization of the text\n",
    "    tokenized_data = nltk.word_tokenize(cleaned_data)\n",
    "    \n",
    "    #Removing Stop words\n",
    "    filtered_words = [word for word in tokenized_data if word not in stopwords.words('english')]\n",
    "    \n",
    "    #All the unigram words are converted to lower case\n",
    "    for i in range(len(filtered_words)):\n",
    "        filtered_words[i] = filtered_words[i]. lower()\n",
    "        \n",
    "    final_words=list()   \n",
    "    for i in filtered_words:    \n",
    "        final_words.append(i)\n",
    "        \n",
    "    #Bigram of words\n",
    "    bigram_data=list(nltk.bigrams(tokenized_data))\n",
    "    for i in bigram_data:\n",
    "        test_string=''\n",
    "        test_string=' '.join(i)\n",
    "        final_words.append(test_string)\n",
    "\n",
    "    #Trigram of words\n",
    "    trigram_data=list(nltk.trigrams(tokenized_data))\n",
    "    for i in trigram_data:\n",
    "        test_string=''\n",
    "        test_string=' '.join(i)\n",
    "        final_words.append(test_string)\n",
    "\n",
    "    #Fourgram of words\n",
    "    fourgram_data=list(nltk.ngrams(tokenized_data,4))\n",
    "    for i in fourgram_data:\n",
    "        test_string=''\n",
    "        test_string=' '.join(i)\n",
    "        final_words.append(test_string)\n",
    "    \n",
    "    #All words are converted to lower case\n",
    "    for i in range(len(final_words)):\n",
    "        final_words[i] = final_words[i]. lower()\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding different types of analytics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_types_analytics(final_words):\n",
    "    \n",
    "    #Collects the previous word of \"analytics\" in the list of words\n",
    "    string=''\n",
    "    for i in range(len(final_words)):\n",
    "        if(final_words[i]==\"analytics\"):\n",
    "            string=string+' '+(str(final_words[i-1]))\n",
    "    \n",
    "    #tokenization\n",
    "    text = nltk.word_tokenize(string)\n",
    "                              \n",
    "    #POS_tagging\n",
    "    #Tells for all the words, whether it is verb, noun, adjective, etc\n",
    "    pos_tagged_text=nltk.pos_tag(text)\n",
    "    tagged_words = nltk.ConditionalFreqDist((tag, word) for (word, tag) in pos_tagged_text)\n",
    "    \n",
    "    #Mentioned the types of noun taggings\n",
    "    noun=['NN','NN$','NN$-HL','NN$-TL','NN-HL','NN-NC','NN-TL','NN-TL-HL','NNS','NNS$','NNS$-HL','NNS$-TL','NNS-HL','NNS-TL','NNS-TL-HL']\n",
    "    \n",
    "    #Collects the previous words of 'analytics' which are noun\n",
    "    final_analytics=[]\n",
    "    for i in range(len(noun)):\n",
    "        for key in tagged_words[noun[i]].keys():\n",
    "            final_analytics.append(key)\n",
    "    \n",
    "    #Appending those noun words like 'data', 'predictive' with 'analytics' ==> 'data analytics', 'predictive analytics'\n",
    "    for i in range(len(final_analytics)):\n",
    "        final_analytics[i]=str(final_analytics[i])+' '+\"analytics\"\n",
    "    return final_analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting languages from a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def languages(final_words):\n",
    "    #from the list of words, collects the languages\n",
    "    language=[]\n",
    "    for i in range(len(final_words)):\n",
    "        for j in range(len(language_list)):\n",
    "            if(final_words[i]==language_list[j]):\n",
    "                language.append(language_list[j])\n",
    "                \n",
    "    #converting it into unique list\n",
    "    language=set(language)\n",
    "    language=list(language)\n",
    "    #if(len(language)==0):\n",
    "    #    return \"No languages mentioned\"\n",
    "    #else:\n",
    "    return language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding difference between R language and R&D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffbet_Rlang_RndD(final_words):\n",
    "    r_count=0\n",
    "    rd_count=0\n",
    "    for i in range(len(final_words)):\n",
    "        if(final_words[i]==\"r\"):\n",
    "            r_count=r_count+1\n",
    "            if(i==len(final_words)-1):\n",
    "                break;\n",
    "            elif(final_words[i+1]=='d'):\n",
    "                rd_count=rd_count+1\n",
    "    if(rd_count>0):\n",
    "        if(r_count==rd_count):\n",
    "            final_words.remove(\"r\")\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting list of skills from the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_skill_list(concluded_words,unique_skill_list):\n",
    "    matching_skills=[]\n",
    "    for i in range(len(concluded_words)):\n",
    "        for j in range(len(unique_skill_list)):\n",
    "            if(concluded_words[i]==unique_skill_list[j]):\n",
    "                matching_skills.append(concluded_words[i])\n",
    "    \n",
    "    matching_skills=set(matching_skills)\n",
    "    matching_skills=list(matching_skills)\n",
    "    return matching_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the incorrectly mentioned skill and appending the skill list with correct name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_wrong_namings(matching_skills,wrong,correct):\n",
    "    for i in range(len(matching_skills)):\n",
    "        for j in range(len(wrong)):\n",
    "            if(matching_skills[i]==wrong[j]):\n",
    "                matching_skills.remove(wrong[j])\n",
    "                matching_skills.append(correct[j])\n",
    "    \n",
    "    matching_skills=set(matching_skills)\n",
    "    matching_skills=list(matching_skills)\n",
    "    return matching_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding difference between 'analytics' and different types of analytics like 'data analytics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_skills(matching_skills,corrected_skills,final_analytics):\n",
    "    diff_analytics_count=0\n",
    "    analytics_count=0\n",
    "    for i in range(len(matching_skills)):\n",
    "        if(matching_skills[i]==\"analytics\"):\n",
    "            analytics_count=analytics_count+1\n",
    "        for j in range(len(final_analytics)):\n",
    "            if(matching_skills[i]==final_analytics[j]):\n",
    "                diff_analytics_count=diff_analytics_count+1\n",
    "    if(diff_analytics_count>0):\n",
    "        if(analytics_count==diff_analytics_count):\n",
    "            corrected_skills.remove(\"analytics\")\n",
    "    return corrected_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concluding informations from job data by calling the above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#calling the above functions\n",
    "words_job_description=processing(job_description_data)\n",
    "\n",
    "final_analytics_jd=diff_types_analytics(words_job_description)    \n",
    "final_languages_jd=languages(words_job_description)\n",
    "concluded_words_jd=diffbet_Rlang_RndD(words_job_description)\n",
    "\n",
    "matching_skills_jd=matching_skill_list(concluded_words_jd,unique_skill_list)\n",
    "corrected_skills_jd=remove_wrong_namings(matching_skills_jd,wrong,correct)\n",
    "processed_skills_jd=processing_skills(matching_skills_jd,corrected_skills_jd,final_analytics_jd)\n",
    "company_score=3\n",
    "company_score=company_score+len(final_languages_jd)\n",
    "company_score=company_score+len(processed_skills_jd)\n",
    "print(company_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industry Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading industries names file\n",
    "names_data=pd.read_csv(\"industries1.csv\")\n",
    "names_dataframe=pd.DataFrame(names_data)\n",
    "one=names_dataframe['one']\n",
    "two=names_dataframe['two']\n",
    "three=names_dataframe['three']\n",
    "four=names_dataframe['four']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Marketing   and    Advertising']\n"
     ]
    }
   ],
   "source": [
    "#making changes in the words which are wrongly displayed\n",
    "#eg.: SoftwareStaffingRecruiting => Software Staffing Recruting\n",
    "ind=[]\n",
    "\n",
    "string=''\n",
    "for j in industry:\n",
    "    if(j==j.lower()):\n",
    "        string=string+j\n",
    "    if(j==j.upper()):\n",
    "        string = string+\" \"+j\n",
    "ind.append(string)\n",
    "\n",
    "for i in range(len(ind)):\n",
    "    if(\"& &\" in ind[i]):\n",
    "        ind[i]=ind[i].replace(\" & \",' ')\n",
    "    if(\", ,\" in ind[i]):\n",
    "        ind[i]=ind[i].replace(\", ,\",' ')\n",
    "    if(\"/\" in ind[i]):\n",
    "        ind[i]=ind[i].replace(\"/\",' ')\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marketing and advertising\n"
     ]
    }
   ],
   "source": [
    "#tokenization        \n",
    "for i in range(len(ind)):\n",
    "    test=nltk.word_tokenize(ind[i])\n",
    "\n",
    "    #finds the industry type of the company    \n",
    "    if(len(test)==1):\n",
    "        for l in range(len(one)):\n",
    "            if(test[0]==one[l]):\n",
    "                found_industry=str(test[0]).lower()\n",
    "\n",
    "    elif(len(test)==2):\n",
    "        for l in range(len(one)):\n",
    "            if(test[0]==one[l] and test[1]==two[l]):\n",
    "                found_industry=str(test[0]+' '+test[1]).lower()\n",
    "            elif(test[1]==one[l] and two[l]==' '):\n",
    "                found_industry=str(test[1]).lower()\n",
    "\n",
    "    elif(len(test)==3):\n",
    "        for l in range(len(one)):\n",
    "            if(test[0]==one[l] and test[1]==two[l] and test[2]==three[l]):\n",
    "                found_industry=str(test[0]+' '+test[1]+' '+test[2]).lower()\n",
    "            elif(test[1]==one[l] and test[2]==two[l]):\n",
    "                found_industry=str(test[1]+' '+test[2]).lower()\n",
    "            elif(test[2]==one[l]):\n",
    "                found_industry=str(test[2]).lower()\n",
    "\n",
    "    elif(len(test)==0):\n",
    "        continue;\n",
    "\n",
    "    else:\n",
    "        ind1=[]\n",
    "        for j in range(len(test),len(test)-4,-1):\n",
    "            ind1.append(test[j-1])\n",
    "        ind1=ind1[::-1]\n",
    "        for l in range(len(one)):\n",
    "            if(ind1[0]==one[l] and ind1[1]==two[l] and ind1[2]==three[l] and ind1[3]==four[l]):\n",
    "                found_industry=str(ind1[0]+' '+ind1[1]+' '+ind1[2]+' '+ind1[3]).lower()\n",
    "            elif(ind1[1]==one[l] and ind1[2]==two[l] and ind1[3]==three[l]):\n",
    "                found_industry=str(ind1[1]+' '+ind1[2]+' '+ind1[3]).lower()\n",
    "            elif(ind1[2]==one[l] and ind1[3]==two[l]):\n",
    "                found_industry=str(ind1[2]+' '+ind1[3]).lower()\n",
    "            elif(ind1[3]==one[l]):\n",
    "                found_industry=str(ind1[3]).lower()\n",
    "print(found_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputting informations from the resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID                                                    2.01079e+09\n",
      "Date                                                       27-08-2020\n",
      "Company Name                                     CollidaScope Limited\n",
      "Post                                                   Data Scientist\n",
      "Location                              London, England, United Kingdom\n",
      "No.of Applicants                                                  122\n",
      "Description         CollidaScope has an opportunity for a free-thi...\n",
      "Level                                                     Entry level\n",
      "Type                                                        Full-time\n",
      "Function                            EngineeringInformation Technology\n",
      "Industry                                    Marketing and Advertising\n",
      "Link                                                                 \n",
      "Review                                                               \n",
      "Name: 1990, dtype: object\n",
      "\n",
      "Required languages:\n",
      "[]\n",
      "\n",
      "Required skills:\n",
      "['analysis', 'data mining', 'mysql', 'statistics', 'sas', 'power bi', 'data modelling', 'excel', 'machine learning', 'data analytics', 'bi', 'data visualization', 'analytics', 'python', 'project management', 'regression', 'r']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manila']\n",
      "Candidate location: manila\n",
      "Distance in km:\n",
      "2020_CV_Ronn_Kevin_Santos.docx : 10735.678852673145\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "2020_CV_Ronn_Kevin_Santos.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "Aaditya_CV.docx : 5478.478415221937\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Aaditya_CV.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "Abdelrahman-CV-N.docx : 5478.478415221937\n",
      "Languages known : ['arabic']\n",
      "Matching languages : \n",
      "Abdelrahman-CV-N.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "Abdullah_Alattar_2020.docx : 5478.478415221937\n",
      "Languages known : ['arabic']\n",
      "Matching languages : \n",
      "Abdullah_Alattar_2020.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['abu dhabi']\n",
      "Candidate location: abu dhabi\n",
      "Distance in km:\n",
      "Abukersh_Jun2020.docx : 5472.345189615449\n",
      "Languages known : ['italian', 'arabic']\n",
      "Matching languages : \n",
      "Abukersh_Jun2020.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : 0\n",
      "level matches\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "AdityaM.docx : 5478.478415221937\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "AdityaM.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "Afra Yaqoob CV.docx : 5478.478415221937\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Afra Yaqoob CV.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "Agnel Mamachan CV & Cover Letter.docx : 5478.478415221937\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Agnel Mamachan CV & Cover Letter.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : 2\n",
      "level matches\n",
      "\n",
      "\n",
      "['tripoli']\n",
      "Candidate location: tripoli\n",
      "Distance in km:\n",
      "Ahmed El Chafei Resume.docx : 2333.314074724882\n",
      "Languages known : ['dutch', 'arabic', 'german', 'french', 'spanish', 'italian']\n",
      "Matching languages : \n",
      "Ahmed El Chafei Resume.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['abu dhabi']\n",
      "Candidate location: abu dhabi\n",
      "Distance in km:\n",
      "Ahmed Hassan-CV-Resume-August 2020.docx : 5472.345189615449\n",
      "Languages known : ['arabic']\n",
      "Matching languages : \n",
      "Ahmed Hassan-CV-Resume-August 2020.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['karachi']\n",
      "Candidate location: karachi\n",
      "Distance in km:\n",
      "Ahmed Nurullah_BI and Data Lead.docx : 6298.164499583673\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Ahmed Nurullah_BI and Data Lead.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : 11\n",
      "level does not match\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "Ahmed_Abdelkader_CV-2.docx : 5478.478415221937\n",
      "Languages known : ['arabic']\n",
      "Matching languages : \n",
      "Ahmed_Abdelkader_CV-2.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['indianapolis']\n",
      "Candidate location: indianapolis\n",
      "Distance in km:\n",
      "Aizaz CV 2.01 (4).docx : 6420.193387009308\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Aizaz CV 2.01 (4).docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['chennai']\n",
      "Candidate location: chennai\n",
      "Distance in km:\n",
      "ajmal_resume.docx : 8211.97733499229\n",
      "Languages known : ['hindi', 'arabic']\n",
      "Matching languages : \n",
      "ajmal_resume.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : 12\n",
      "level does not match\n",
      "\n",
      "\n",
      "['mumbai']\n",
      "Candidate location: mumbai\n",
      "Distance in km:\n",
      "Alia_cv_final.docx : 7200.25174889281\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Alia_cv_final.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : 18\n",
      "level does not match\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "Alma-Resume.docx : 5478.478415221937\n",
      "Languages known : ['tamil']\n",
      "Matching languages : \n",
      "Alma-Resume.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['istanbul']\n",
      "Candidate location: istanbul\n",
      "Distance in km:\n",
      "Amine CV-9.docx : 2499.7988198539456\n",
      "Languages known : ['spanish', 'turkish', 'french', 'arabic']\n",
      "Matching languages : \n",
      "Amine CV-9.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['madrid']\n",
      "Candidate location: madrid\n",
      "Distance in km:\n",
      "Antonio Bastidas_Resume_Aug20.docx : 1263.414718393201\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Antonio Bastidas_Resume_Aug20.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : 5\n",
      "level does not match\n",
      "\n",
      "\n",
      "['hong kong']\n",
      "Candidate location: hong kong\n",
      "Distance in km:\n",
      "Ashley_Choy_CV.docx : 9626.277381457214\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Ashley_Choy_CV.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : Not mentioned\n",
      "level matches\n",
      "\n",
      "\n",
      "['dubai']\n",
      "Candidate location: dubai\n",
      "Distance in km:\n",
      "Atif_Ahmad_CV.docx : 5478.478415221937\n",
      "Languages known : []\n",
      "Matching languages : \n",
      "Atif_Ahmad_CV.docx : []\n",
      "Matching Industry Type: industry not found\n",
      "Experience level : 20\n",
      "level does not match\n",
      "\n",
      "\n",
      "['philadelphia']\n",
      "Candidate location: philadelphia\n"
     ]
    }
   ],
   "source": [
    "#skills and languages are collected from the job description\n",
    "print(single_company_data)\n",
    "print(\"\\nRequired languages:\")\n",
    "print(final_languages_jd)\n",
    "print(\"\\nRequired skills:\")\n",
    "print(processed_skills_jd)\n",
    "print(\"\\n\")\n",
    "\n",
    "score=[]\n",
    "industries3=[]\n",
    "languages3=[]\n",
    "locations3=[]\n",
    "years3=[]\n",
    "candidate_skills=[]\n",
    "candidate={}\n",
    "a,b,c,p,z=0,0,0,0,0\n",
    "name=[]\n",
    "\n",
    "cities_data=pd.read_csv(\"worldcities.csv\")\n",
    "cities=cities_data['city_ascii']\n",
    "for i in range(len(cities)):\n",
    "    cities[i]=cities[i].lower()\n",
    "#print(cities)\n",
    "\n",
    "#Reading all the resumes from the 'resumes' folder\n",
    "entries = os.listdir('resumes')\n",
    "\n",
    "#for loop for each resume\n",
    "for entry in entries:\n",
    "    \n",
    "    #converts docs file to text\n",
    "    resume_data = docx2txt.process(entry)\n",
    "    \n",
    "    #name of the candidate is appended in the list 'name'\n",
    "    name.append(entry)\n",
    "    \n",
    "    #calling the above functions\n",
    "    words_resume=processing(resume_data)\n",
    "    \n",
    "    final_analytics_resume=diff_types_analytics(words_resume)\n",
    "    final_languages_resume=languages(words_resume)\n",
    "    concluded_words_resume=diffbet_Rlang_RndD(words_resume)        \n",
    "    \n",
    "    matching_skills_resume=matching_skill_list(concluded_words_resume,unique_skill_list)\n",
    "    corrected_skills_resume=remove_wrong_namings(matching_skills_resume,wrong,correct)\n",
    "    processed_skills_resume=processing_skills(matching_skills_resume,corrected_skills_resume,final_analytics_resume)\n",
    "    \n",
    "    #skills are collected from the resume\n",
    "    candidate_skills.append(processed_skills_resume)\n",
    "    candidate[name[a]]=processed_skills_resume\n",
    "    a=a+1\n",
    "    \n",
    "    places2=[]\n",
    "    #location of the candidate\n",
    "    for i in range(len(cities)):\n",
    "        for j in range(len(words_resume)):\n",
    "            if(cities[i]==words_resume[j]):\n",
    "                places2.append(words_resume[j])\n",
    "                print(places2)\n",
    "                if(len(places2)==0):\n",
    "                    print(\"goto\")\n",
    "                    continue;\n",
    "                else:\n",
    "                    break;\n",
    "                break;\n",
    "            else: \n",
    "                continue; \n",
    "            break;\n",
    "        else: \n",
    "            continue; \n",
    "        break;\n",
    "    candidate_location=places2[0]\n",
    "    candidate_location=str(candidate_location)\n",
    "    print(\"Candidate location:\",candidate_location)\n",
    "    \n",
    "    places=[]\n",
    "    places.append(company_location)\n",
    "    places.append(candidate_location)\n",
    "    latitude=[]\n",
    "    longitude=[]\n",
    "      \n",
    "    #finding the distance between the candidate location and the company's location\n",
    "    geolocator = Nominatim(user_agent=\"http\")\n",
    "    for i in range(len(places)):\n",
    "        locate = geolocator.geocode(places[i])\n",
    "        latitude.append(locate.latitude)\n",
    "        longitude.append(locate.longitude)\n",
    "           \n",
    "    first = (latitude[0], longitude[0])\n",
    "    second = (latitude[1], longitude[1])\n",
    "    print(\"Distance in km:\")\n",
    "    print(name[b],\":\",great_circle(first, second).km)\n",
    "    location_string=candidate_location+' '+str(great_circle(first, second).km)\n",
    "    scores=0\n",
    "    if(int(great_circle(first, second).km)<=50):\n",
    "        scores=scores+1\n",
    "    elif(int(great_circle(first, second).km)>50):\n",
    "        scores=scores\n",
    "    #print(scores)\n",
    "    score.append(scores)\n",
    "    #print(score)\n",
    "    b=b+1   \n",
    "    locations3.append(location_string)\n",
    "    \n",
    "    #collects the languages known by the candidate\n",
    "    #if(len(final_languages_resume)==0):\n",
    "    #    print(\"No languages mentioned\")\n",
    "    #else:\n",
    "    print(\"Languages known :\",final_languages_resume)    \n",
    "    \n",
    "    #finds the matching languages with the job description and the candidate's resume\n",
    "    matching_languages=[]\n",
    "    for i in range(len(final_languages_jd)):\n",
    "        for j in range(len(final_languages_resume)):\n",
    "            if(final_languages_jd[i]==final_languages_resume[j]):\n",
    "                matching_languages.append(final_languages_jd[i])\n",
    "    print(\"Matching languages : \")\n",
    "    print(name[c],\":\",matching_languages)\n",
    "    \n",
    "    language_string=''\n",
    "    for lang in range(len(matching_languages)):\n",
    "        language_string=language_string+' '+str(matching_languages[lang])\n",
    "    #print(len(matching_languages))\n",
    "    score[c]=score[c]+len(matching_languages)\n",
    "    #print(score)\n",
    "    c=c+1\n",
    "    if language_string=='':\n",
    "        language_string=\"none\"\n",
    "    languages3.append(language_string)\n",
    "    \n",
    "    #industry type matching\n",
    "    industry_score=0\n",
    "    industri=' '\n",
    "    for i in range(len(words_resume)):\n",
    "        if(words_resume[i]==found_industry):\n",
    "            industry_score=1\n",
    "            industri=found_industry\n",
    "            #print(industri)\n",
    "            \n",
    "    if(industri==' '):\n",
    "        industri=\"industry not found\"\n",
    "        industry_score=0\n",
    "        \n",
    "    if(industry_score==1):\n",
    "        score[z]=score[z]+1\n",
    "    elif(industry_score==0):\n",
    "        score[z]=score[z]\n",
    "    #print(industry_score)\n",
    "    #print(score)\n",
    "    z=z+1\n",
    "    print(\"Matching Industry Type:\",industri)\n",
    "    industries3.append(industri)\n",
    "    \n",
    "    \n",
    "    #level of experience matching\n",
    "    years=[]\n",
    "    for j in range(len(words_resume)):\n",
    "        if(words_resume[j]=='years'):\n",
    "            try:\n",
    "                years.append(int(words_resume[j-1]))\n",
    "            except:\n",
    "                years.append(0)\n",
    "                \n",
    "    years1=[]\n",
    "    \n",
    "    if(len(years)>0):\n",
    "        for y in range(len(years)):\n",
    "            if(years[y]<=20):\n",
    "                years1.append(years[y])\n",
    "        \n",
    "        if(len(years1)>0):           \n",
    "            maxx=years1[0]\n",
    "            for i in range(0, len(years1)):        \n",
    "                if(years1[i] > maxx):    \n",
    "                    maxx = years1[i] \n",
    "            #year = sum(filter(lambda m: isinstance(m, int), years))\n",
    "            year=maxx\n",
    "            #year=int(''.join(list(filter(lambda c: c.isdigit(), year))))\n",
    "            print(\"Experience level :\",year)\n",
    "\n",
    "        else:\n",
    "            year=0\n",
    "            print(\"Experience level : Not mentioned\")\n",
    "    else:\n",
    "        year=0\n",
    "        print(\"Experience level : Not mentioned\")\n",
    "        \n",
    "    \n",
    "    # Job position's needed level        candidate's years of experience  \n",
    "    #-------------------------------------------------------------------\n",
    "    # Not Applicable                =>   greater than or equal to 0\n",
    "    # Entry level or Associate      =>   greater than or equal to 0\n",
    "    # Mid-Senior level              =>   greater than 3\n",
    "    # Executive                     =>   greater than 5\n",
    "    # Director                      =>   greater than 7\n",
    "    \n",
    "    if(type(level)==str):\n",
    "        if(level==\"Not Applicable\"):\n",
    "            if(year>=0):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "\n",
    "        elif(level==\"Entry level\" or level==\"Associate\"):\n",
    "            if(3>year>=0):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "            else:\n",
    "                print(\"level does not match\")\n",
    "                level_score=0\n",
    "\n",
    "        elif(level==\"Mid-Senior level\"):\n",
    "            if(5>=year>=3):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "            else:\n",
    "                print(\"level does not match\")\n",
    "                level_score=0\n",
    "\n",
    "        elif(level==\"Executive\"):\n",
    "            if(7>=year>=5):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "            else:\n",
    "                print(\"level does not match\")  \n",
    "                level_score=0\n",
    "\n",
    "        elif(level==\"Director\"):\n",
    "            if(year>=7):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "            else:\n",
    "                print(\"level does not match\") \n",
    "                level_score=0\n",
    "        #print(\"\\n\")\n",
    "        \n",
    "    elif(type(level)==int):\n",
    "        if(level==' '):\n",
    "            if(year>=0):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "\n",
    "        elif(3>int(level)>=0):\n",
    "            if(3>year>=0):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "            else:\n",
    "                print(\"level does not match\")\n",
    "                level_score=0\n",
    "\n",
    "        elif(5>=int(level)>=3):\n",
    "            if(5>=year>=3):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "            else:\n",
    "                print(\"level does not match\")\n",
    "                level_score=0\n",
    "\n",
    "        elif(7>=int(level)>=5):\n",
    "            if(7>=year>=5):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "            else:\n",
    "                print(\"level does not match\")  \n",
    "                level_score=0\n",
    "\n",
    "        elif(int(level)>=7):\n",
    "            if(year>=7):\n",
    "                print(\"level matches\")\n",
    "                level_score=1\n",
    "            else:\n",
    "                print(\"level does not match\") \n",
    "                level_score=0\n",
    "                \n",
    "    if(level_score==1):\n",
    "        level_string=\"level matches\"\n",
    "    elif(level_score==0):\n",
    "        level_string=\"level not matches\"       \n",
    "    years3.append(level_string)  \n",
    "    #print(level_score)\n",
    "    score[p]=score[p]+level_score\n",
    "    #print(score)\n",
    "    p=p+1\n",
    "    print(\"\\n\")\n",
    "print(score)        \n",
    "print(industries3)\n",
    "print(locations3)\n",
    "print(languages3)\n",
    "print(years3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching skills with the job description and the candidate's resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ranking : \n",
      "\n",
      "2020_CV_Ronn_Kevin_Santos.docx  :  1\n",
      "Aaditya_CV.docx  :  3\n",
      "Abdelrahman-CV-N.docx  :  1\n",
      "Abdullah_Alattar_2020.docx  :  3\n",
      "Abukersh_Jun2020.docx  :  0\n",
      "AdityaM.docx  :  5\n",
      "Afra Yaqoob CV.docx  :  1\n",
      "Agnel Mamachan CV & Cover Letter.docx  :  6\n",
      "Ahmed El Chafei Resume.docx  :  0\n",
      "Ahmed Hassan-CV-Resume-August 2020.docx  :  1\n",
      "Ahmed Nurullah_BI and Data Lead.docx  :  3\n",
      "Ahmed_Abdelkader_CV-2.docx  :  0\n",
      "Aizaz CV 2.01 (4).docx  :  3\n",
      "ajmal_resume.docx  :  1\n",
      "Alia_cv_final.docx  :  1\n",
      "Alma-Resume.docx  :  2\n",
      "Amine CV-9.docx  :  1\n",
      "Antonio Bastidas_Resume_Aug20.docx  :  4\n",
      "Ashley_Choy_CV.docx  :  3\n",
      "Atif_Ahmad_CV.docx  :  3\n",
      "Ayesha Cv -.docx  :  4\n",
      "Bachir Barry Data Science CV.docx  :  4\n",
      "Balgopal_Sabat CV.docx  :  1\n",
      "Bamidele Olanrewaju Ajayi-converted.docx  :  0\n",
      "Beatriz Manzano CV.docx  :  4\n",
      "BI Developer- Anupam Parti.docx  :  1\n",
      "Bushra's Resume.docx  :  5\n",
      "CV - DA.docx  :  3\n",
      "CV - Shalaka Kumar (1).docx  :  0\n",
      "CV-2020 Mohammed Al Balushi.docx  :  0\n",
      "CV-John-Richard-Gonzales-n.docx  :  0\n",
      "CV-Mohamed Salem.docx  :  0\n",
      "CV_Ammara1.docx  :  3\n",
      "CV_Mohamed Anvergani_4.docx  :  4\n",
      "CV_Omar_Daoudi_.docx  :  4\n",
      "CV_Ranimaria.docx  :  1\n",
      "Dheemantha Wijesinghe - CV.docx  :  2\n",
      "DIALA ALMALIK CV.docx  :  1\n",
      "DS-2008.docx  :  3\n",
      "EA-CV_Sameh_BenFredj_2020.docx  :  5\n",
      "EA-Resume Anton Dimov.docx  :  0\n",
      "Geoffrey Brown.docx  :  2\n",
      "LyanneGibson.docx  :  2\n",
      "MallikaCV (1).docx  :  5\n",
      "Mandrekar_Isha.docx  :  6\n",
      "Matt McCarthy - CV.docx  :  5\n",
      "MhdAlaa-CV-Updated-2.docx  :  2\n",
      "Reeka Hazarika - Resume.docx  :  3\n",
      "Resume Sajjad Tahir Nov 2019.docx  :  3\n",
      "Shireen-CV-2020.docx  :  4\n",
      "\n",
      "Matching skills : \n",
      "\n",
      "2020_CV_Ronn_Kevin_Santos.docx  :  {'python'}\n",
      "Aaditya_CV.docx  :  {'cloud', 'python', 'nlp'}\n",
      "Abdelrahman-CV-N.docx  :  {'python'}\n",
      "Abdullah_Alattar_2020.docx  :  {'python', 'keras', 'tensorflow'}\n",
      "Abukersh_Jun2020.docx  :  set()\n",
      "AdityaM.docx  :  {'statistics', 'keras', 'data modelling', 'python', 'tensorflow'}\n",
      "Afra Yaqoob CV.docx  :  {'python'}\n",
      "Agnel Mamachan CV & Cover Letter.docx  :  {'keras', 'data modelling', 'nlp', 'analytics', 'python', 'tensorflow'}\n",
      "Ahmed El Chafei Resume.docx  :  set()\n",
      "Ahmed Hassan-CV-Resume-August 2020.docx  :  {'python'}\n",
      "Ahmed Nurullah_BI and Data Lead.docx  :  {'cloud', 'python', 'statistics'}\n",
      "Ahmed_Abdelkader_CV-2.docx  :  set()\n",
      "Aizaz CV 2.01 (4).docx  :  {'cloud', 'analytics', 'statistics'}\n",
      "ajmal_resume.docx  :  {'analytics'}\n",
      "Alia_cv_final.docx  :  {'analytics'}\n",
      "Alma-Resume.docx  :  {'probability', 'python'}\n",
      "Amine CV-9.docx  :  {'python'}\n",
      "Antonio Bastidas_Resume_Aug20.docx  :  {'nlp', 'analytics', 'python', 'statistics'}\n",
      "Ashley_Choy_CV.docx  :  {'analytics', 'python', 'statistics'}\n",
      "Atif_Ahmad_CV.docx  :  {'analytics', 'python', 'statistics'}\n",
      "Ayesha Cv -.docx  :  {'cloud', 'python', 'keras', 'tensorflow'}\n",
      "Bachir Barry Data Science CV.docx  :  {'python', 'analytics', 'data modelling', 'statistics'}\n",
      "Balgopal_Sabat CV.docx  :  {'python'}\n",
      "Bamidele Olanrewaju Ajayi-converted.docx  :  set()\n",
      "Beatriz Manzano CV.docx  :  {'probability', 'analytics', 'data modelling', 'statistics'}\n",
      "BI Developer- Anupam Parti.docx  :  {'python'}\n",
      "Bushra's Resume.docx  :  {'keras', 'pytorch', 'analytics', 'python', 'tensorflow'}\n",
      "CV - DA.docx  :  {'analytics', 'python', 'statistics'}\n",
      "CV - Shalaka Kumar (1).docx  :  set()\n",
      "CV-2020 Mohammed Al Balushi.docx  :  set()\n",
      "CV-John-Richard-Gonzales-n.docx  :  set()\n",
      "CV-Mohamed Salem.docx  :  set()\n",
      "CV_Ammara1.docx  :  {'nlp', 'python', 'statistics'}\n",
      "CV_Mohamed Anvergani_4.docx  :  {'cloud', 'analytics', 'data modelling', 'statistics'}\n",
      "CV_Omar_Daoudi_.docx  :  {'python', 'cloud', 'data modelling', 'statistics'}\n",
      "CV_Ranimaria.docx  :  {'analytics'}\n",
      "Dheemantha Wijesinghe - CV.docx  :  {'analytics', 'statistics'}\n",
      "DIALA ALMALIK CV.docx  :  {'analytics'}\n",
      "DS-2008.docx  :  {'nlp', 'python', 'tensorflow'}\n",
      "EA-CV_Sameh_BenFredj_2020.docx  :  {'keras', 'cloud', 'analytics', 'python', 'tensorflow'}\n",
      "EA-Resume Anton Dimov.docx  :  set()\n",
      "Geoffrey Brown.docx  :  {'python', 'statistics'}\n",
      "LyanneGibson.docx  :  {'python', 'statistics'}\n",
      "MallikaCV (1).docx  :  {'data modelling', 'cloud', 'analytics', 'python', 'statistics'}\n",
      "Mandrekar_Isha.docx  :  {'tensorflow', 'keras', 'data modelling', 'nlp', 'python', 'statistics'}\n",
      "Matt McCarthy - CV.docx  :  {'data modelling', 'cloud', 'analytics', 'python', 'statistics'}\n",
      "MhdAlaa-CV-Updated-2.docx  :  {'cloud', 'python'}\n",
      "Reeka Hazarika - Resume.docx  :  {'tensorflow', 'python', 'statistics'}\n",
      "Resume Sajjad Tahir Nov 2019.docx  :  {'analytics', 'statistics', 'data modelling'}\n",
      "Shireen-CV-2020.docx  :  {'python', 'analytics', 'data modelling', 'statistics'}\n",
      "[1, 3, 1, 3, 0, 5, 1, 6, 0, 1, 3, 0, 3, 1, 1, 2, 1, 5, 3, 3, 4, 5, 1, 0, 4, 1, 6, 3, 0, 0, 0, 0, 3, 5, 4, 1, 3, 1, 3, 5, 0, 2, 2, 5, 6, 5, 2, 3, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "final_rank={}\n",
    "matching={}\n",
    "def fun(candidate,comp,st_n):\n",
    "\n",
    "    set1=set(candidate)\n",
    "    set2=set(comp)\n",
    "    \n",
    "    set3=set1.intersection(set2)\n",
    "    \n",
    "    final_rank.update({st_n:len(set3)})\n",
    "\n",
    "    matching.update({st_n:set3})\n",
    "    \n",
    "for i,j in candidate.items():\n",
    "    fun(processed_skills_jd,j,i)\n",
    "     \n",
    "#Final ranking tells the count of the matching skills \n",
    "#print(\"\\nFinal ranking : \" ,final_rank)\n",
    "print(\"Final ranking : \\n\")\n",
    "i=0\n",
    "for key, value in final_rank.items():\n",
    "    print(key, ' : ', value)\n",
    "    score[i]=score[i]+value\n",
    "    i=i+1\n",
    "\n",
    "#Matching skills\n",
    "#print('Matching skills : ' , matching)\n",
    "print('\\nMatching skills : \\n')\n",
    "for key1, value1 in matching.items():\n",
    "    print(key1, ' : ', value1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python'], ['cloud', 'python', 'nlp'], ['python'], ['python', 'keras', 'tensorflow'], [], ['statistics', 'keras', 'data modelling', 'python', 'tensorflow'], ['python'], ['keras', 'data modelling', 'nlp', 'analytics', 'python', 'tensorflow'], [], ['python'], ['cloud', 'python', 'statistics'], [], ['cloud', 'analytics', 'statistics'], ['analytics'], ['analytics'], ['probability', 'python'], ['python'], ['nlp', 'analytics', 'python', 'statistics'], ['analytics', 'python', 'statistics'], ['analytics', 'python', 'statistics'], ['cloud', 'python', 'keras', 'tensorflow'], ['python', 'analytics', 'data modelling', 'statistics'], ['python'], [], ['probability', 'analytics', 'data modelling', 'statistics'], ['python'], ['keras', 'pytorch', 'analytics', 'python', 'tensorflow'], ['analytics', 'python', 'statistics'], [], [], [], [], ['nlp', 'python', 'statistics'], ['cloud', 'analytics', 'data modelling', 'statistics'], ['python', 'cloud', 'data modelling', 'statistics'], ['analytics'], ['analytics', 'statistics'], ['analytics'], ['nlp', 'python', 'tensorflow'], ['keras', 'cloud', 'analytics', 'python', 'tensorflow'], [], ['python', 'statistics'], ['python', 'statistics'], ['data modelling', 'cloud', 'analytics', 'python', 'statistics'], ['tensorflow', 'keras', 'data modelling', 'nlp', 'python', 'statistics'], ['data modelling', 'cloud', 'analytics', 'python', 'statistics'], ['cloud', 'python'], ['tensorflow', 'python', 'statistics'], ['analytics', 'statistics', 'data modelling'], ['python', 'analytics', 'data modelling', 'statistics']]\n"
     ]
    }
   ],
   "source": [
    "names1=[]\n",
    "for key1, value1 in matching.items():\n",
    "    names1.append(key1)\n",
    "#print(names1)\n",
    "ski1=[]\n",
    "for key1, value1 in matching.items():\n",
    "    ski1.append(value1)\n",
    "for i in range(len(ski1)):\n",
    "    ski1[i]=list(ski1[i])\n",
    "print(ski1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] ['industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found', 'industry not found'] ['mumbai 7200.25174889281', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'paris 343.5072691492576', 'madrid 1263.414718393201', 'london 0.0', 'london 0.0', 'jaipur 6751.861567545736', 'hyderabad 7719.7640521729845', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'philadelphia 5699.690886663006', 'miami 7126.016062776013', 'dubai 5478.478415221937', 'mumbai 7200.25174889281', 'lahore 6282.352078939211', 'lahore 6282.352078939211', 'karachi 6298.164499583673', 'indianapolis 6420.193387009308', 'hyderabad 7719.7640521729845', 'hong kong 9626.277381457214', 'enterprise 7059.359483858228', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'san francisco 8616.093762050956', 'paris 343.5072691492576', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'mysore 8027.285349716462', 'mumbai 7200.25174889281', 'manila 10735.678852673145', 'istanbul 2499.7988198539456', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'dubai 5478.478415221937', 'chennai 8211.97733499229', 'atlanta 6770.1074696358255', 'abu dhabi 5472.345189615449', 'tripoli 2333.314074724882', 'saint petersburg 2091.23140059856', 'moscow 2500.5982675695764', 'kano 259.0420608764735', 'dubai 5478.478415221937', 'delhi 6709.121972691988', 'davao 11713.00331724515', 'cairo 3511.391510253383', 'abu dhabi 5472.345189615449'] ['none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none'] ['level not matches', 'level not matches', 'level matches', 'level not matches', 'level matches', 'level not matches', 'level not matches', 'level not matches', 'level matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches', 'level not matches'] [['tensorflow', 'keras', 'data modelling', 'nlp', 'python', 'statistics'], ['keras', 'data modelling', 'nlp', 'analytics', 'python', 'tensorflow'], ['keras', 'pytorch', 'analytics', 'python', 'tensorflow'], ['keras', 'cloud', 'analytics', 'python', 'tensorflow'], ['nlp', 'analytics', 'python', 'statistics'], ['python', 'analytics', 'data modelling', 'statistics'], ['cloud', 'analytics', 'data modelling', 'statistics'], ['data modelling', 'cloud', 'analytics', 'python', 'statistics'], ['python', 'analytics', 'data modelling', 'statistics'], ['statistics', 'keras', 'data modelling', 'python', 'tensorflow'], ['data modelling', 'cloud', 'analytics', 'python', 'statistics'], ['cloud', 'python', 'keras', 'tensorflow'], ['probability', 'analytics', 'data modelling', 'statistics'], ['python', 'cloud', 'data modelling', 'statistics'], ['nlp', 'python', 'tensorflow'], ['nlp', 'python', 'statistics'], ['analytics', 'statistics', 'data modelling'], ['cloud', 'python', 'statistics'], ['cloud', 'analytics', 'statistics'], ['analytics', 'python', 'statistics'], ['analytics', 'python', 'statistics'], ['analytics', 'statistics'], ['tensorflow', 'python', 'statistics'], ['python', 'keras', 'tensorflow'], ['cloud', 'python', 'nlp'], ['analytics', 'python', 'statistics'], ['python', 'statistics'], ['python', 'statistics'], ['probability', 'python'], ['cloud', 'python'], ['analytics'], ['analytics'], ['python'], ['python'], ['python'], ['python'], ['python'], ['analytics'], ['analytics'], ['python'], ['python'], [], [], [], [], [], [], [], [], []] ['Mandrekar_Isha.docx', 'Agnel Mamachan CV & Cover Letter.docx', \"Bushra's Resume.docx\", 'EA-CV_Sameh_BenFredj_2020.docx', 'Antonio Bastidas_Resume_Aug20.docx', 'Bachir Barry Data Science CV.docx', 'CV_Mohamed Anvergani_4.docx', 'MallikaCV (1).docx', 'Shireen-CV-2020.docx', 'AdityaM.docx', 'Matt McCarthy - CV.docx', 'Ayesha Cv -.docx', 'Beatriz Manzano CV.docx', 'CV_Omar_Daoudi_.docx', 'DS-2008.docx', 'CV_Ammara1.docx', 'Resume Sajjad Tahir Nov 2019.docx', 'Ahmed Nurullah_BI and Data Lead.docx', 'Aizaz CV 2.01 (4).docx', 'CV - DA.docx', 'Ashley_Choy_CV.docx', 'Dheemantha Wijesinghe - CV.docx', 'Reeka Hazarika - Resume.docx', 'Abdullah_Alattar_2020.docx', 'Aaditya_CV.docx', 'Atif_Ahmad_CV.docx', 'Geoffrey Brown.docx', 'LyanneGibson.docx', 'Alma-Resume.docx', 'MhdAlaa-CV-Updated-2.docx', 'CV_Ranimaria.docx', 'Alia_cv_final.docx', '2020_CV_Ronn_Kevin_Santos.docx', 'Amine CV-9.docx', 'BI Developer- Anupam Parti.docx', 'Afra Yaqoob CV.docx', 'Abdelrahman-CV-N.docx', 'DIALA ALMALIK CV.docx', 'ajmal_resume.docx', 'Balgopal_Sabat CV.docx', 'Ahmed Hassan-CV-Resume-August 2020.docx', 'Ahmed El Chafei Resume.docx', 'CV-2020 Mohammed Al Balushi.docx', 'EA-Resume Anton Dimov.docx', 'Bamidele Olanrewaju Ajayi-converted.docx', 'Ahmed_Abdelkader_CV-2.docx', 'CV - Shalaka Kumar (1).docx', 'CV-John-Richard-Gonzales-n.docx', 'CV-Mohamed Salem.docx', 'Abukersh_Jun2020.docx']\n"
     ]
    }
   ],
   "source": [
    "zipped = list(zip(*sorted(zip(score,industries3,locations3,languages3,years3,ski1,names1))))\n",
    "score10,industries10,locations10,languages10,years10,ski10,names10 = [ list(tuple) for tuple in zipped]\n",
    "\n",
    "score10.reverse()\n",
    "industries10.reverse()\n",
    "locations10.reverse()\n",
    "languages10.reverse()\n",
    "years10.reverse()\n",
    "ski10.reverse()\n",
    "names10.reverse()\n",
    "print(score10,industries10,locations10,languages10,years10,ski10,names10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies2=[]\n",
    "companies2.append(title)\n",
    "companies2.append(1992)\n",
    "companies2.append(found_industry)\n",
    "companies2.append(final_languages_jd)\n",
    "companies2.append(company_location)\n",
    "companies2.append(level)\n",
    "companies2.append(processed_skills_jd)\n",
    "comp_len=len(companies2)\n",
    "for i in range(len(score10)-comp_len):\n",
    "    companies2.append(' ')\n",
    "#print(companies2)\n",
    "\n",
    "properties2=[]\n",
    "properties2.append(company_score)\n",
    "for i in range(len(score10)-1):\n",
    "    properties2.append(' ')\n",
    "#print(properties2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes3=pd.DataFrame({'Company':companies2,\n",
    "                      'Property':properties2,\n",
    "                      'Names':names10,\n",
    "                      'Score':score10,\n",
    "                      'Industry':industries10,\n",
    "                      'Language':languages10,\n",
    "                      'Location':locations10,\n",
    "                      'Experience level':years10,\n",
    "                      'Skills':ski10})\n",
    "      \n",
    "#storing in csv file\n",
    "resumes3.to_csv(\"Output.csv\", mode='a', header=False, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
